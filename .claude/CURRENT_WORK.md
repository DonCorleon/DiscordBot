# Current Work: Pluggable Speech Engines

**Branch:** `feat-pluggable-speech-engines`
**Last Updated:** 2025-10-30
**Status:** In Progress - Speech engine abstraction complete, cog split pending

---

## üìç Current Focus

**Status**: Implementing pluggable speech recognition system

**Goal**: Separate voice connection logic from speech recognition, allowing swappable engines (Vosk, Whisper, etc.) without changing voice management code.

---

## ‚úÖ Completed This Session (2025-10-30)

### 1. Speech Engine Abstraction System

Created modular speech recognition at `bot/core/audio/speech_engines/`:

```
bot/core/audio/speech_engines/
‚îú‚îÄ‚îÄ __init__.py          # Factory: create_speech_engine()
‚îú‚îÄ‚îÄ base.py              # Abstract SpeechEngine class
‚îú‚îÄ‚îÄ vosk.py              # VoskEngine (production-ready)
‚îú‚îÄ‚îÄ whisper.py           # WhisperEngine (stub for future)
‚îî‚îÄ‚îÄ config.py            # SpeechConfig schema
```

**Features:**
- Abstract `SpeechEngine` interface: `start_listening()`, `stop_listening()`, `get_sink()`
- `VoskEngine` - Extracted from existing voice_speech.py, fully functional
- `WhisperEngine` - Stub with TODOs (user has working example to implement)
- Factory function enables engine swapping via config
- Ducking callback support for speaking events
- Error handling and logging built-in

**Usage:**
```python
# Create engine (Vosk or Whisper)
engine = create_speech_engine(bot, transcription_callback, "vosk", ducking_callback)

# Start listening
await engine.start_listening(voice_client)

# Callback receives clean transcription
def transcription_callback(member, text):
    # Handle text (soundboard, logging, etc.)
    pass
```

### 2. Logging Cleanup

Reduced log verbosity for cleaner INFO level output:

**Before:**
```
[INFO] [Soundboards] [696940351977422878] [The_KnobFather] : hello
[INFO] [1410579846379081801] Found 1 sound(s) for: 'hello'
[INFO] [TRANSCRIPTION] {"timestamp": "...", "guild_id": ..., "text": "hello", "triggers": [...]}
```

**After (INFO level):**
```
[INFO] [TRANSCRIPTION] The_KnobFather: hello ‚Üí triggers: hello
```

**After (DEBUG level):**
```
[DEBUG] [Soundboards] [696940351977422878] [The_KnobFather] : hello
[DEBUG] [1410579846379081801] Found 1 sound(s) for: 'hello'
[DEBUG] [TRANSCRIPTION_DEBUG] {"timestamp": "...", "guild_id": ..., "text": "hello", "triggers": [...]}
[INFO] [TRANSCRIPTION] The_KnobFather: hello ‚Üí triggers: hello
```

**Files Modified:**
- `bot/cogs/audio/voice_speech.py` - Minimal INFO log, detailed DEBUG log
- `bot/cogs/audio/soundboard.py` - "Found X sounds" moved to DEBUG

---

## üîÑ Pending Tasks

### Next: Split voice_speech.py into voice.py + speech.py

**Target Structure:**
```
bot/cogs/audio/
‚îú‚îÄ‚îÄ voice.py              # Voice connection, queue, ducking
‚îú‚îÄ‚îÄ speech.py             # Speech recognition (uses engines)
‚îî‚îÄ‚îÄ soundboard.py         # Existing
```

**voice.py** (Voice Connection & Playback):
- Voice client management (join/leave/auto-join)
- Sound playback queue processor
- Audio ducking (start/stop speaking handlers)
- Keepalive loop
- Commands: `~join`, `~leave`, `~unjoin`, `~autojoin`, `~ducking`, `~play`, `~queue`, `~clearqueue`

**speech.py** (Speech Recognition):
- Instantiates speech engines via factory
- Handles transcription callbacks (soundboard triggers, logging, data collection)
- Manages transcript sessions
- Commands: `~start`, `~stop` (start/stop listening)
- Gets voice cog reference for ducking/queueing: `bot.get_cog("VoiceCog")`

**Integration:**
- Speech ‚Üí Voice: Ducking events, sound queueing
- Voice ‚Üí Speech: Independent (no coupling)

### Implementation Steps:

1. **Register SpeechConfig** schema in bot startup
2. **Create voice.py** cog:
   - Extract voice connection logic
   - Extract sound queue processor
   - Extract ducking handlers
   - Extract voice commands
3. **Create speech.py** cog:
   - Use speech engine factory
   - Handle transcriptions
   - Coordinate with voice.py
4. **Update main.py** to load both cogs
5. **Test** end-to-end functionality
6. **Commit** when stable

---

## üéØ Design Decisions

### Why Split voice_speech.py?

**Problem:** Monolithic 1,200-line file mixing:
- Voice connection management
- Speech recognition
- Sound playback queuing
- Audio ducking
- Transcript management

**Solution:** Split by domain:
- **voice.py** = "How do I connect to Discord voice and play sounds?"
- **speech.py** = "How do I recognize speech and trigger actions?"

**Benefits:**
- Swap speech engines without touching voice logic
- Test voice playback independently of speech
- Clearer separation of concerns
- Easier to add new engines (Whisper, Google STT, etc.)

### Why Speech Engine Abstraction?

User has working Whisper example with:
- Custom audio buffering (rolling 3-second buffer)
- Per-user transcription tasks
- Better accuracy than Vosk (but higher latency)

**Goal:** Support both engines:
- **Vosk** - Fast, local, good for real-time
- **Whisper** - Accurate, slower, good for quality

Switch via config without code changes.

---

## üî® Technical Details

### Speech Engine Interface

```python
class SpeechEngine(ABC):
    def __init__(self, bot, callback):
        self.bot = bot
        self.callback = callback  # (member, text) -> None

    @abstractmethod
    async def start_listening(self, voice_client):
        """Attach to voice client and start recognition"""
        pass

    @abstractmethod
    async def stop_listening(self):
        """Stop recognition and cleanup"""
        pass

    @abstractmethod
    def get_sink(self):
        """Return voice_recv sink if applicable"""
        pass
```

### VoskEngine Implementation

- Wraps existing `SpeechRecognitionSink` from voice_recv
- Parses Vosk JSON output: `{"text": "hello"}`
- Error handling for corrupted audio data
- Speaking event callbacks for ducking integration
- ~10 lines of speech recognition code, rest is integration

### WhisperEngine (Stub)

- Placeholder for future implementation
- User's working example shows:
  - `ResilientWhisperSink` (custom BasicSink)
  - Per-user audio buffers (dict)
  - Rolling buffer (keep last 3 seconds)
  - Background transcription tasks
  - Resampling (96kHz ‚Üí 16kHz for Whisper)
  - Debouncing (1 second between transcriptions)

---

## üì¶ Files Changed (Not Committed Yet)

### New Files:
- `bot/core/audio/speech_engines/__init__.py`
- `bot/core/audio/speech_engines/base.py`
- `bot/core/audio/speech_engines/vosk.py`
- `bot/core/audio/speech_engines/whisper.py`
- `bot/core/audio/speech_engines/config.py`

### Modified Files:
- `bot/cogs/audio/voice_speech.py` - Logging cleanup
- `bot/cogs/audio/soundboard.py` - Logging cleanup

**All files compile successfully.**

---

## üß™ Testing Plan

### Phase 1: Test Speech Engine Abstraction
1. Register SpeechConfig schema
2. Update voice_speech.py to use VoskEngine
3. Verify transcription still works
4. Verify ducking still works
5. Verify soundboard triggers still work

### Phase 2: Test Cog Split
1. Create voice.py and speech.py
2. Test voice connection (join/leave)
3. Test sound playback queue
4. Test speech recognition
5. Test integration (ducking, soundboard triggers)

### Phase 3: Polish
1. Update documentation
2. Add WhisperEngine implementation (optional)
3. Test engine swapping via config

---

## üìù Session Notes

### Token Usage
- Started: ~56k/200k
- Current: ~121k/200k
- Remaining: ~79k (enough to continue but monitor)

### User Preferences
- Wants minimal INFO logs, detailed DEBUG logs
- Prefers concise explanations (not verbose)
- Wants ability to swap speech engines easily
- Referenced working Whisper example from separate project

### Branch Management
- Deleted `refactor-file-splitting` branch (over-engineered)
- Created `feat-pluggable-speech-engines` from master
- Master has latest features (PacketRouter fix, transcript persistence)

---

## üîÑ Session Recovery

**If session interrupted, resume with:**

```
Current branch: feat-pluggable-speech-engines
Status: Speech engine abstraction complete, ready for cog split

Completed:
‚úÖ Speech engine abstraction (base, vosk, whisper stub, factory)
‚úÖ Logging cleanup (INFO minimal, DEBUG detailed)
‚úÖ All code compiles

Next steps:
1. Register SpeechConfig in bot startup
2. Split voice_speech.py into voice.py + speech.py
3. Test integration
4. Commit when stable

Key files:
- bot/core/audio/speech_engines/ (5 new files)
- bot/cogs/audio/voice_speech.py (logging changes)
- bot/cogs/audio/soundboard.py (logging changes)
```

---

**Document Version**: 7.0 (Pluggable Speech Engines)
**Last Updated By**: Claude (2025-10-30)
**Next Review**: After completing cog split and testing
